{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SBPR_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMuuy5gFAIOiSP+7vdAgIeq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WideSu/CS608RecommendationSystem/blob/main/SBPR_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "-BKUVdYATz1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Github/SerenRec/\n",
        "data_dir = './data/'\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv(data_dir+'train_data.csv')\n",
        "test_data = pd.read_csv(data_dir+'test_data.csv')\n",
        "# sys.path.append(\"./seren/data/\")\n",
        "from seren.data.Preprocess import *\n",
        "from seren.data.dataset import * "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBA-ULC7Ty4M",
        "outputId": "c4ea9f99-384d-47c1-cc2a-a9ed61630575"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Github/SerenRec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocess()\n",
        "train_seq = preprocessor.to_sequence(df = train_data)\n",
        "test_seq = preprocessor.to_sequence(df = test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hD1dt--aRTX",
        "outputId": "f8f1d9a2-c59f-4ab5-c232-4b27d77323ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SessionDataset(\n",
        "    seq_list = train_seq['sequence'], \n",
        "    next_list= train_seq['next'], sample_cnt=0,item_id_map = {11943: 0, 1: 1})"
      ],
      "metadata": {
        "id": "-Y4dk76Hb8A5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SessionDataset(\n",
        "    seq_list = test_seq['sequence'], \n",
        "    next_list= test_seq['next'], \n",
        "    sample_cnt=0,\n",
        "    item_id_map = {11943: 0, 1: 1})"
      ],
      "metadata": {
        "id": "jEfdcB2KfFcj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_data)\n",
        "test_dataloader = DataLoader(test_data)"
      ],
      "metadata": {
        "id": "_UaO7SPffefX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_num = train_data['item_id'].nunique()\n",
        "session_num = len(train_seq)"
      ],
      "metadata": {
        "id": "2zt9zvNwrGbw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_seq, next_item, _ = next(train_dataset.get_loader())"
      ],
      "metadata": {
        "id": "P1QcuuIUtAOZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "@article{ludewig2018evaluation,\n",
        "  title={Evaluation of session-based recommendation algorithms},\n",
        "  author={Ludewig, Malte and Jannach, Dietmar},\n",
        "  journal={User Modeling and User-Adapted Interaction},\n",
        "  volume={28},\n",
        "  number={4},\n",
        "  pages={331--390},\n",
        "  year={2018},\n",
        "  publisher={Springer}\n",
        "}\n",
        "@article{rendle2012bpr,\n",
        "  title={BPR: Bayesian personalized ranking from implicit feedback},\n",
        "  author={Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and Schmidt-Thieme, Lars},\n",
        "  journal={arXiv preprint arXiv:1205.2618},\n",
        "  year={2012}\n",
        "}\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SBPR(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        Session-BPR Recommender\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        item_num : int\n",
        "            the number of unique items in training set\n",
        "        embedding_dim : int\n",
        "            embedding dimension for items, default is 100\n",
        "        lambda_item : float\n",
        "            l2-regularization term for item embedding, default is 0.001\n",
        "        learning_rate : float\n",
        "            learning tate, default is 0.01\n",
        "        weight_decay : float\n",
        "            weight decaying rate for learning rate, default is 1.0\n",
        "        n_epoch : int\n",
        "            epochs for training, default is 20\n",
        "        early_stop : bool\n",
        "            activate early stop mechanism or not, default is True\n",
        "        max_len : int \n",
        "            maximum length for one session\n",
        "        device : String\n",
        "            running type for code, default is 'cpu'\n",
        "        learner : String\n",
        "            name of optimizer used for training, default is 'sgd'\n",
        "        '''     \n",
        "        super(SBPR, self).__init__() \n",
        "        self.item_num = config['item_num']\n",
        "        self.embedding_dim = config['embedding_dim']\n",
        "        self.lambda_item = config['lambda_item']\n",
        "        self.lr = config['learning_rate']\n",
        "        self.wd = config['weight_decay']\n",
        "        self.n_epoch = config['n_epoch']\n",
        "        self.early_stop = config['early_stop']\n",
        "        self.max_len = config['max_len']\n",
        "        self.device = config['device']\n",
        "        self.learner = config['learner']\n",
        "\n",
        "        self.item_embed = nn.Embedding(self.item_num+1, self.embedding_dim, padding_idx=0)\n",
        "        \n",
        "        self.apply(self._init_weight)\n",
        "    \n",
        "    def _init_weight(self, m):        \n",
        "        if type(m) == nn.Embedding:\n",
        "            nn.init.normal_(m.weight.data, 0, 0.05)\n",
        "            with torch.no_grad():\n",
        "                m.weight[0] = torch.zeros(self.embedding_dim)\n",
        "\n",
        "    def _select_optimizer(self, **kwargs):\n",
        "        params = kwargs.pop('params', self.parameters())\n",
        "        learner = kwargs.pop('learner', self.learner)\n",
        "        learning_rate = kwargs.pop('learning_rate', self.lr)\n",
        "        weight_decay = kwargs.pop('weight_decay', self.wd)\n",
        "\n",
        "        if learner.lower() == 'adam':\n",
        "            optimizer = optim.Adam(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif learner.lower() == 'sgd':\n",
        "            optimizer = optim.SGD(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif learner.lower() == 'adagrad':\n",
        "            optimizer = optim.Adagrad(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif learner.lower() == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        else:\n",
        "            self.logger.warning('Invalid optimizer name, set default SGD optimizer instead')\n",
        "            optimizer = optim.SGD(params, lr=learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, item_seq, next_item):\n",
        "        next_item_embed = self.item_embed(next_item)\n",
        "        session_items_embed =  self.item_embed(item_seq)\n",
        "        # torch.count_nonzero(item_seq, dim = 1) # 1-D tensor for each batch\n",
        "        session_embed = torch.div(\n",
        "            torch.sum(session_items_embed,dim=1), \n",
        "            torch.count_nonzero(item_seq, dim=1).reshape(-1,1)) # shape: batch * max_len\n",
        "        next_item_score = (session_embed * next_item_embed).sum(dim=-1)\n",
        "        \n",
        "        return next_item_score\n",
        "    \n",
        "    def fit(self, train_loader):\n",
        "        self.to(self.device)\n",
        "        optimizer = self._select_optimizer(learning_rate=self.lr, weight_decay=self.wd)\n",
        "        \n",
        "        last_loss = 0.\n",
        "        for epoch in range(1, self.n_epoch + 1):\n",
        "            self.train()\n",
        "            current_loss, sample_cnt = 0, 0\n",
        "            pbar = tqdm(train_loader)\n",
        "            pbar.set_description(f'[Epoch {epoch:03d}]')\n",
        "            for item_seq, pos_next_item, neg_next_item in pbar:\n",
        "                item_seq = item_seq.to(self.device)\n",
        "                pos_next_item = pos_next_item.to(self.device)\n",
        "                neg_next_item = neg_next_item.to(self.device)\n",
        "                self.zero_grad()\n",
        "                r_si = self.forward(item_seq, pos_next_item)\n",
        "                r_sj = self.forward(item_seq, neg_next_item)\n",
        "                loss = -(r_si - r_sj).sigmoid().log().mean() + self.lambda_item * self.item_embed.weight.norm()\n",
        "                if torch.isnan(loss):\n",
        "                    raise ValueError(f'Loss=Nan or Infinity: current settings does not fit the recommender')\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                current_loss += loss.item()\n",
        "                sample_cnt += 1\n",
        "        \n",
        "            current_loss /= sample_cnt\n",
        "\n",
        "            self.eval()\n",
        "            delta_loss = float(current_loss - last_loss)\n",
        "            if (abs(delta_loss) < 1e-5) and self.early_stop:\n",
        "                print('Satisfy early stop mechanism')\n",
        "                break\n",
        "            else:\n",
        "                last_loss = current_loss\n",
        "    \n",
        "    def predict(self, input_ids, next_item):\n",
        "        if len(input_ids) > self.max_len or len(input_ids) == 0:\n",
        "            raise ValueError('Invalid sequence length to predict, current supported maximum length is {self.max_len}...')\n",
        "\n",
        "        self.eval()\n",
        "        item_seq = torch.tensor(input_ids).to(self.device)\n",
        "        item_seq = F.pad(item_seq, (0,self.max_len - len(input_ids))).unsqueeze(0)\n",
        "\n",
        "        next_item = torch.tensor(next_item).to(self.device)\n",
        "        score = self.forward(item_seq, next_item)\n",
        "        return score.detach().cpu().item()\n",
        "\n",
        "    def rank(self, test_loader, topk=50):\n",
        "        self.eval()\n",
        "        res_ids,res_scs = torch.tensor([]).to(self.device),torch.tensor([]).to(self.device)\n",
        "        pbar = tqdm(test_loader)\n",
        "        with torch.no_grad():\n",
        "            for btch in pbar:\n",
        "                item_seq = btch[0]\n",
        "                item_seq = item_seq.to(self.device)\n",
        "                session_items_embed =  self.item_embed(item_seq)\n",
        "                session_embed = torch.div(\n",
        "                    torch.sum(session_items_embed,dim=1), # batch_num * embed_dim\n",
        "                    torch.count_nonzero(item_seq, dim=1).reshape(-1,1)) # shape: batch * embed_dim\n",
        "                all_item_embs = self.item_embed.weight\n",
        "                scores = torch.matmul(session_embed, all_item_embs.transpose(0, 1))\n",
        "                scs, ids = torch.sort(scores[:, 1:], descending=True)\n",
        "                ids += 1\n",
        "\n",
        "                if topk is not None and topk <= self.item_num:\n",
        "                    ids, scs = ids[:, :topk], scs[:, :topk]\n",
        "\n",
        "                res_ids = torch.cat((res_ids, ids), 0)\n",
        "                res_scs = torch.cat((res_scs, scs), 0)\n",
        "\n",
        "        return res_ids.detach().cpu(), res_scs.detach().cpu()\n"
      ],
      "metadata": {
        "id": "nWpBzcJEB9dt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# sys.path.append(\"./seren/model/\")\n",
        "# from seren.model.sbpr import SBPRobject\n",
        "config = {\n",
        "    'item_num' : item_num,\n",
        "    'embedding_dim' : 100,\n",
        "    'lambda_item' : 0.001,\n",
        "    'learning_rate' : 0.001,\n",
        "    'weight_decay' : 1,\n",
        "    'n_epoch' : 20,\n",
        "    'early_stop' : True,\n",
        "    'max_len' : 5,\n",
        "    'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'learner' : 'sgd'\n",
        "}\n",
        "sbpr = SBPR(config)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
        "sbpr.fit(train_dataloader)"
      ],
      "metadata": {
        "id": "ag_xay4mgAVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22ff019-9167-483a-b4a2-98fc78d4739d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 001]: 100%|██████████| 3602/3602 [00:28<00:00, 127.10it/s, loss=0.695]\n",
            "[Epoch 002]: 100%|██████████| 3602/3602 [00:28<00:00, 127.88it/s, loss=0.693]\n",
            "[Epoch 003]: 100%|██████████| 3602/3602 [00:28<00:00, 128.01it/s, loss=0.693]\n",
            "[Epoch 004]: 100%|██████████| 3602/3602 [00:27<00:00, 129.51it/s, loss=0.693]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satisfy early stop mechanism\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=512)"
      ],
      "metadata": {
        "id": "ujt77w5M1mdf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_ids, res_scs = sbpr.rank(test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv26D0PtyfQ6",
        "outputId": "0fa233d8-33ed-40c1-95d5-3f29f2ab57c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 46.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4PlA6NHJkq3",
        "outputId": "46383b42-b2c7-41da-b266-f3f0d6e26124"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20188, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}