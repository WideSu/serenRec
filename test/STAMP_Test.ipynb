{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "STAMP_Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPcL5OjPK/u1uZAE6FP1ovq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WideSu/CS608RecommendationSystem/blob/main/STAMP_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "-BKUVdYATz1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Github/SerenRec/\n",
        "data_dir = './data/'\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv(data_dir+'train_data.csv')\n",
        "test_data = pd.read_csv(data_dir+'test_data.csv')\n",
        "# sys.path.append(\"./seren/data/\")\n",
        "from seren.data.Preprocess import *\n",
        "from seren.data.dataset import * "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBA-ULC7Ty4M",
        "outputId": "5da1b7c7-b168-43f6-f506-0ce238bc2486"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Github/SerenRec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocess()\n",
        "train_seq = preprocessor.to_sequence(df = train_data)\n",
        "test_seq = preprocessor.to_sequence(df = test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hD1dt--aRTX",
        "outputId": "a87f07cf-9aa9-40dd-c753-fa2b882b8c36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SessionDataset(\n",
        "    seq_list = train_seq['sequence'], \n",
        "    next_list= train_seq['next'], sample_cnt=0,item_id_map = {11943: 0, 1: 1})"
      ],
      "metadata": {
        "id": "-Y4dk76Hb8A5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SessionDataset(\n",
        "    seq_list = test_seq['sequence'], \n",
        "    next_list= test_seq['next'], \n",
        "    sample_cnt=0,\n",
        "    item_id_map = {11943: 0, 1: 1})"
      ],
      "metadata": {
        "id": "jEfdcB2KfFcj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(train_data)\n",
        "test_dataloader = DataLoader(test_data)"
      ],
      "metadata": {
        "id": "_UaO7SPffefX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_num = train_data['item_id'].nunique()\n",
        "session_num = len(train_seq)"
      ],
      "metadata": {
        "id": "2zt9zvNwrGbw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_seq, next_item, _ = next(train_dataset.get_loader())"
      ],
      "metadata": {
        "id": "P1QcuuIUtAOZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "@inproceedings{liu2018stamp,\n",
        "  title={STAMP: short-term attention/memory priority model for session-based recommendation},\n",
        "  author={Liu, Qiao and Zeng, Yifu and Mokhosi, Refuoe and Zhang, Haibin},\n",
        "  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n",
        "  pages={1831--1839},\n",
        "  year={2018}\n",
        "}\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class STAMP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''\n",
        "        STAMP Recommender\n",
        "        Parameters\n",
        "        ----------\n",
        "        embedding_dim : int\n",
        "            embedding dimension for items, default is 100\n",
        "        mlp_a_dim : int\n",
        "            dimension for the hidden layer of MLP A, default is the same as `embedding_dim`\n",
        "        mlp_b_dim : int\n",
        "            dimension for the hidden layer of MLP B, default is the same as `embedding_dim`\n",
        "        learning_rate : float\n",
        "            learning rate, default is 0.005\n",
        "        weight_decay : float\n",
        "            weight decaying rate for learning rate, default is 1.0\n",
        "        n_epoch : int\n",
        "            epochs for training, default is 30\n",
        "        early_stop : bool\n",
        "            activate early stop mechanism or not, default is True\n",
        "        learner : String\n",
        "            name of optimizaer used for training, default is 'sgd'\n",
        "        device : String\n",
        "            running type for code, default is 'cpu'\n",
        "        max_len : int\n",
        "            maximum length for one session\n",
        "        use_attention : bool\n",
        "            use STAMP or STMP, default is True (for STAMP)\n",
        "        item_num : int\n",
        "            the number of unique items in training set\n",
        "        loss_type : String\n",
        "            the loss function type, default is CE\n",
        "        '''              \n",
        "        super(STAMP, self).__init__()\n",
        "        self.embedding_dim = config['embedding_dim']\n",
        "        self.lr = config['learning_rate']\n",
        "        self.wd = config['weight_decay']  \n",
        "        self.n_epoch = config['n_epoch'] \n",
        "        self.early_stop = config['early_stop']\n",
        "        self.learner = config['learner']\n",
        "        self.device = config['device']\n",
        "        self.max_len = config['max_len']\n",
        "        self.item_num = config['item_num']\n",
        "        self.loss_type = config['loss_type']\n",
        "\n",
        "        self.mlp_a_dim = self.embedding_dim if config['mlp_a_dim'] is None else config['mlp_a_dim']\n",
        "        self.mlp_b_dim = self.embedding_dim if config['mlp_b_dim'] is None else config['mlp_b_dim']\n",
        "\n",
        "        self.item_embedding = nn.Embedding(self.item_num + 1, self.embedding_dim, padding_idx=0)\n",
        "        self.mlp_a = nn.Linear(self.embedding_dim, self.mlp_a_dim)\n",
        "        self.mlp_b = nn.Linear(self.embedding_dim, self.mlp_b_dim)\n",
        "\n",
        "        # attention related\n",
        "        self.W_0 = nn.Linear(self.embedding_dim, 1, bias=False)\n",
        "        self.W_1 = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n",
        "        self.W_2 = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n",
        "        self.W_3 = nn.Linear(self.embedding_dim, self.embedding_dim, bias=False)\n",
        "        self.b_a = nn.Parameter(torch.zeros(self.embedding_dim), requires_grad=True)\n",
        "\n",
        "        # activate function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "        self.apply(self._init_weight)\n",
        "\n",
        "        self.use_attention = config['use_attention'] # True for STAMP, o.w. STMP\n",
        "\n",
        "    def _init_weight(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            nn.init.normal_(m.weight.data, 0, 0.05)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.)\n",
        "        elif type(m) == nn.Embedding:\n",
        "            nn.init.normal_(m.weight.data, 0, 0.002)\n",
        "            with torch.no_grad():\n",
        "                m.weight[0] = torch.zeros(self.embedding_dim)\n",
        "\n",
        "    def forward(self, item_seq):\n",
        "        item_seq_len = torch.count_nonzero(item_seq, dim=1)\n",
        "        last_index = item_seq_len - 1\n",
        "        # batch_size * seq_len * embedding_dim\n",
        "        x_i = self.item_embedding(item_seq) \n",
        "        # batch_size * 1 * embedding_dim, x_t = m_t\n",
        "        x_t = x_i.gather(\n",
        "            dim=1, index=last_index.view(-1, 1, 1).expand(-1, -1 ,x_i.shape[-1])).squeeze(1)\n",
        "        m_s = torch.div(torch.sum(x_i, dim=1), item_seq_len.unsqueeze(1).float())\n",
        "        \n",
        "        if self.use_attention:\n",
        "            # attention score\n",
        "            alpha = self._calc_att_score(x_i, x_t, m_s)\n",
        "            m_a = torch.matmul(alpha.unsqueeze(1), x_i).squeeze(1)\n",
        "            m_s = m_a + m_s\n",
        "\n",
        "        h_s = self.tanh(self.mlp_a(m_s))\n",
        "        h_t = self.tanh(self.mlp_b(x_t))\n",
        "        output = h_s * h_t\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _calc_att_score(self, x_i, x_t, m_s):\n",
        "        timesteps = x_i.size(1)\n",
        "        x_t_reshape = x_t.repeat(1, timesteps).view(-1, timesteps, self.embedding_dim)\n",
        "        m_s_reshape = m_s.repeat(1, timesteps).view(-1, timesteps, self.embedding_dim)\n",
        "\n",
        "        alpha = self.W_0(self.sigmoid(\n",
        "            self.W_1(x_i) + self.W_2(x_t_reshape) + self.W_3(m_s_reshape) + self.b_a))\n",
        "        alpha = alpha.squeeze(2)\n",
        "        return alpha\n",
        "\n",
        "    def _select_optimizer(self, **kwargs):\n",
        "        params = kwargs.pop('params', self.parameters())\n",
        "        learner = kwargs.pop('learner', self.learner)\n",
        "        learning_rate = kwargs.pop('learning_rate', self.lr)\n",
        "        weight_decay = kwargs.pop('weight_decay', self.wd)\n",
        "\n",
        "        if learner.lower() == 'adam':\n",
        "            optimizer = optim.Adam(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif learner.lower() == 'sgd':\n",
        "            optimizer = optim.SGD(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif learner.lower() == 'adagrad':\n",
        "            optimizer = optim.Adagrad(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        elif learner.lower() == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(params, lr=learning_rate, weight_decay=weight_decay)\n",
        "        else:\n",
        "            self.logger.warning('Invalid optimizer name, set default SGD optimizer instead')\n",
        "            optimizer = optim.SGD(params, lr=learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def fit(self, train_loader):\n",
        "        self.to(self.device)\n",
        "        # calculate loss\n",
        "        optimizer = self._select_optimizer(learning_rate=self.lr, weight_decay=self.wd)\n",
        "\n",
        "        last_loss = 0.\n",
        "        for epoch in range(1, self.n_epoch + 1):\n",
        "            self.train()\n",
        "\n",
        "            current_loss, sample_cnt = 0., 0\n",
        "            pbar = tqdm(train_loader)\n",
        "            pbar.set_description(f'[Epoch {epoch:03d}]')\n",
        "            for item_seq, pos_next_item, neg_next_item in pbar:\n",
        "                item_seq = item_seq.to(self.device)\n",
        "                pos_next_item = pos_next_item.to(self.device)\n",
        "                self.zero_grad()\n",
        "                output = self.forward(item_seq)\n",
        "                if self.loss_type in ['CE']:\n",
        "                    logits = self.sigmoid(torch.matmul(output, self.item_embedding.weight.transpose(0, 1)))\n",
        "                    pred_y = self.softmax(logits)\n",
        "                    criterion = nn.CrossEntropyLoss()\n",
        "                    loss = criterion(pred_y, pos_next_item)\n",
        "                elif self.loss_type in ['BPR', 'TOP1']:\n",
        "                    neg_next_item = neg_next_item.to(self.device)\n",
        "                    pos_items_emb = self.item_embedding(pos_next_item)\n",
        "                    neg_items_emb = self.item_embedding(neg_next_item)\n",
        "                    pos_score = torch.sum(output * pos_items_emb, dim=-1)  # [B]\n",
        "                    neg_score = torch.sum(output * neg_items_emb, dim=-1)  # [B]\n",
        "                    loss = -torch.log(1e-10 + torch.sigmoid(pos_score - neg_score)).mean()\n",
        "                else:\n",
        "                    raise ValueError(f'Invalid Loss type: {self.loss_type}...')\n",
        "\n",
        "                if torch.isnan(loss):\n",
        "                    raise ValueError(f'Loss=Nan or Infinity: current settings does not fit the recommender')\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "                current_loss += loss.item()\n",
        "                sample_cnt += 1\n",
        "\n",
        "            current_loss /= sample_cnt\n",
        "\n",
        "            self.eval()\n",
        "            delta_loss = float(current_loss - last_loss)\n",
        "            if (abs(delta_loss) < 1e-5) and self.early_stop:\n",
        "                print('Satisfy early stop mechanism')\n",
        "                break\n",
        "            else:\n",
        "                last_loss = current_loss\n",
        "\n",
        "\n",
        "    def predict(self, input_ids, next_item):   \n",
        "        '''\n",
        "        method to predict the score of a target item given certain session items basket\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_ids : List\n",
        "            a list of items in certain session\n",
        "        next_item : int\n",
        "            the index of the target next item\n",
        "        Returns\n",
        "        -------\n",
        "        scores : float\n",
        "            predicted scores of corresponding target items\n",
        "        '''      \n",
        "        if len(input_ids) > self.max_len or len(input_ids) == 0:\n",
        "            raise ValueError(f'Invalid sequence length to predict, current supported maximum length is {self.max_len}...')\n",
        "\n",
        "        self.eval()\n",
        "        item_seq = torch.tensor(input_ids).to(self.device)\n",
        "        item_seq = F.pad(item_seq, (0, self.max_len - len(input_ids))).unsqueeze(0)\n",
        "        next_item = torch.tensor(next_item).to(self.device)\n",
        "\n",
        "        seq_output = self.forward(item_seq)\n",
        "        next_item_emb = self.item_embedding(next_item)\n",
        "        score = torch.mul(seq_output, next_item_emb).sum(dim=1) \n",
        "\n",
        "        return score.detach().cpu().item()\n",
        "\n",
        "    def rank(self, test_loader,topk=50):\n",
        "        \"\"\"_summary_\n",
        "        Args:\n",
        "            test_loader (_type_): _description_\n",
        "            topk (int, optional): _description_. Defaults to 50.\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"        \n",
        "        self.eval()\n",
        "\n",
        "        res_ids, res_scs = torch.tensor([]).to(self.device), torch.tensor([]).to(self.device)\n",
        "        pbar = tqdm(test_loader)\n",
        "        with torch.no_grad():\n",
        "            for btch in pbar:\n",
        "                item_seq = btch[0]\n",
        "                item_seq = item_seq.to(self.device)\n",
        "                output = self.forward(item_seq)\n",
        "                logits = self.sigmoid(torch.matmul(output, self.item_embedding.weight.transpose(0, 1)))\n",
        "                scores = self.softmax(logits)\n",
        "                scs, ids = torch.sort(scores[:, 1:], descending=True)\n",
        "                ids += 1\n",
        "\n",
        "                if topk is not None and topk <= self.item_num:\n",
        "                    ids, scs = ids[:, :topk], scs[:, :topk]\n",
        "\n",
        "                res_ids = torch.cat((res_ids, ids), 0)\n",
        "                res_scs = torch.cat((res_scs, scs), 0)\n",
        "\n",
        "        return res_ids.detach().cpu(), res_scs.detach().cpu()"
      ],
      "metadata": {
        "id": "iIRBhU_wRciX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# sys.path.append(\"./seren/model/\")\n",
        "# from seren.model.stamp import STAMP\n",
        "config = {\n",
        "    'embedding_dim' : 100,\n",
        "    'mlp_a_dim' : 100,\n",
        "    'mlp_b_dim' : 100,\n",
        "    'learning_rate' : 0.001,\n",
        "    'weight_decay' : 1,\n",
        "    'n_epoch' : 30,\n",
        "    'early_stop' : True,\n",
        "    'max_len' : 5,\n",
        "    'use_attention' : True,\n",
        "    'item_num' : item_num,\n",
        "    'device' : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    'learner' : 'sgd', \n",
        "    'loss_type': 'CE'\n",
        "}\n",
        "stamp = STAMP(config)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
        "stamp.fit(train_dataloader)"
      ],
      "metadata": {
        "id": "ag_xay4mgAVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1deb24b-32fd-4866-b9d6-6033c839b11e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch 001]:   0%|          | 0/3602 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "[Epoch 001]: 100%|██████████| 3602/3602 [00:35<00:00, 101.74it/s, loss=9.39]\n",
            "[Epoch 002]: 100%|██████████| 3602/3602 [00:35<00:00, 102.22it/s, loss=9.39]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satisfy early stop mechanism\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(test_dataset, batch_size=512)"
      ],
      "metadata": {
        "id": "ujt77w5M1mdf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_ids, res_scs = stamp.rank(test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv26D0PtyfQ6",
        "outputId": "97f157c1-ab31-467c-9296-35444d199753"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:243: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 40/40 [00:00<00:00, 52.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4PlA6NHJkq3",
        "outputId": "5f040f1a-a621-463f-a47e-c9abfc144a09"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20188, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}